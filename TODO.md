# üìã TODO - KR Lead Scraper

## üî¥ Kritisch (Vor Production)

### Testing
- [x] Unit Tests f√ºr alle Scraper schreiben (99 Tests, 100% Pass Rate) ‚úÖ
- [x] Integration Tests f√ºr API Endpoints ‚úÖ
- [x] Tests f√ºr Data Processors (Validator, Normalizer, Deduplicator) ‚úÖ
- [ ] Tests f√ºr Crawl4AI + Ollama Integration
- [ ] End-to-End Tests f√ºr komplette Pipeline
- [ ] Load Testing f√ºr API
- [x] Test Coverage auf min. 80% erh√∂hen (Scraper & Validator Module) ‚úÖ

### Sicherheit
- [x] API Authentication implementieren (JWT) ‚úÖ
- [x] Rate Limiting f√ºr API Endpoints ‚úÖ
- [x] Input Validation f√ºr alle Endpoints ‚úÖ
- [x] SQL Injection Prevention √ºberpr√ºfen (SQLAlchemy ORM) ‚úÖ
- [x] CORS Konfiguration f√ºr Production ‚úÖ
- [ ] Secrets Management (z.B. AWS Secrets Manager)
- [x] Security Audit durchf√ºhren (Bandit, alle Workflows gr√ºn) ‚úÖ

### Database
- [x] Erste Alembic Migration erstellen (`make db-migrate`)
- [x] psycopg3 Integration (SQLAlchemy 2.0+)
- [x] Indizes f√ºr h√§ufige Queries optimieren ‚úÖ
- [ ] Database Backup Strategy implementieren
- [ ] Connection Pooling testen
- [ ] Query Performance optimieren

### Monitoring & Logging
- [x] Structured Logging implementieren (JSON) ‚úÖ
- [x] Error Tracking (Sentry Integration) ‚úÖ
- [ ] Metrics Collection (Prometheus)
- [ ] Grafana Dashboards erstellen
- [ ] Alerting Setup (Email/Slack)
- [ ] Log Rotation konfigurieren

### CI/CD & DevOps
- [x] GitHub Actions Workflows (Tests, Code Quality, Security) ‚úÖ
- [x] Docker Build Pipeline ‚úÖ
- [x] Branch Protection Rules ‚úÖ
- [x] Python 3.13 Kompatibilit√§t ‚úÖ
- [x] Code Quality Tools (Black, Ruff, isort, mypy) ‚úÖ
- [ ] Production Deployment Pipeline
- [ ] Staging Environment Setup

## üü° Wichtig (N√§chste 2 Wochen)

### API Features
- [ ] Pagination f√ºr alle List-Endpoints testen
- [ ] Filtering & Sorting verbessern
- [x] Bulk Operations (Bulk Update, Bulk Delete) ‚úÖ
- [x] Export Endpoints (CSV, JSON) ‚úÖ
- [x] Webhook Support f√ºr Job Completion ‚úÖ
- [ ] API Versioning Strategy
- [ ] API Documentation verbessern

### Scraping
- [ ] Alle Scraper mit neuen Processors integrieren
- [ ] Smart Scraper in Pipeline integrieren
- [ ] Scraping Job Queue implementieren (Celery/RQ)
- [ ] Retry Logic f√ºr failed Jobs
- [ ] Scraping Statistics Dashboard
- [ ] Rate Limiting pro Source
- [ ] CAPTCHA Handling verbessern

### Data Quality
- [x] Lead Scoring Algorithmus implementieren ‚úÖ
- [ ] Duplicate Detection automatisieren
- [ ] Data Enrichment Pipeline
- [ ] Email Verification Service
- [ ] Phone Number Validation verbessern
- [ ] Website Availability Check
- [ ] Company Size Estimation

### AI/ML Features
- [ ] Ollama Models benchmarken (llama3.2 vs mistral vs qwen2.5)
- [ ] Prompt Engineering optimieren
- [ ] Extraction Accuracy messen
- [ ] Fine-tuning f√ºr Business-Daten evaluieren
- [ ] Fallback-Strategien testen
- [ ] AI Response Caching

## üü¢ Nice-to-Have (Sp√§ter)

### Frontend
- [ ] Admin Dashboard (React/Vue)
- [ ] Company Management UI
- [ ] Scraping Job Management UI
- [ ] Statistics & Analytics Dashboard
- [ ] Export/Import UI
- [ ] User Management

### Integrations
- [ ] SmartWe CRM Integration
- [ ] HubSpot Integration
- [ ] Salesforce Integration
- [ ] Email Marketing Tools (Mailchimp, SendGrid)
- [ ] Slack Notifications
- [ ] Zapier Integration

### Advanced Features
- [ ] Multi-Tenancy Support
- [ ] User Roles & Permissions
- [ ] Audit Log
- [ ] Scheduled Scraping Jobs (Cron)
- [ ] Real-time Scraping Status (WebSockets)
- [ ] Advanced Search (Elasticsearch)
- [ ] GraphQL API

### Performance
- [ ] Caching Strategy (Redis)
- [ ] Database Query Optimization
- [ ] Async Processing f√ºr lange Tasks
- [ ] CDN f√ºr Static Assets
- [ ] Database Sharding evaluieren
- [ ] Read Replicas f√ºr Reporting

### Documentation
- [ ] API Documentation (OpenAPI/Swagger)
- [ ] Architecture Documentation
- [ ] Deployment Guide
- [ ] User Manual
- [ ] Contributing Guidelines
- [ ] Code Comments verbessern

## üß™ Testing Checklist

### Sofort testen
- [ ] `make setup` - Komplettes Setup
- [ ] `make docker-up` - Docker Services starten
- [ ] `make ollama-setup` - Ollama Models installieren
- [ ] `make db-init` - Database initialisieren
- [ ] `make run` - API starten
- [ ] `make health` - Health Check
- [ ] API Endpoints manuell testen (Swagger UI)

### Scraper testen
- [ ] 11880 Scraper mit echten Daten
- [ ] Gelbe Seiten Scraper
- [ ] Das √ñrtliche Scraper
- [ ] GoYellow Scraper
- [ ] Handelsregister Scraper
- [ ] Google Places Integration
- [ ] Smart Scraper mit verschiedenen Websites

### AI Features testen
- [ ] Crawl4AI Extraktion Qualit√§t
- [ ] Ollama Response Zeit
- [ ] Verschiedene Models vergleichen
- [ ] Fallback-Mechanismen
- [ ] Error Handling

### Data Processing testen
- [ ] Email Validation
- [ ] Phone Number Normalization
- [ ] Website URL Cleaning
- [ ] Company Name Normalization
- [ ] Duplicate Detection
- [ ] Merge Logic

### Performance testen
- [ ] API Response Times
- [ ] Database Query Performance
- [ ] Concurrent Scraping Jobs
- [ ] Memory Usage
- [ ] Docker Container Resources

## üì¶ Deployment Checklist

### Pre-Deployment
- [ ] Alle Tests gr√ºn
- [ ] Code Review durchgef√ºhrt
- [ ] Security Scan bestanden
- [ ] Performance Tests OK
- [ ] Database Backup erstellt
- [ ] Rollback Plan dokumentiert

### Production Setup
- [ ] VPS/Cloud Server provisionen
- [ ] Domain & SSL Zertifikat
- [ ] Firewall Regeln
- [ ] Monitoring Setup
- [ ] Backup Strategy
- [ ] CI/CD Pipeline testen

### Post-Deployment
- [ ] Health Checks verifizieren
- [ ] Logs √ºberpr√ºfen
- [ ] Performance Monitoring
- [ ] Error Tracking
- [ ] User Acceptance Testing

## üêõ Known Issues

### Zu beheben
- [ ] Markdown Linting Warnings in README.md
- [ ] MyPy Type Hints vervollst√§ndigen
- [ ] Playwright Browser Download in Docker optimieren
- [ ] Redis Password in Production Docker Compose
- [ ] Nginx Config f√ºr Production erstellen

### Zu evaluieren
- [ ] Crawl4AI vs. Trafilatura Performance
- [ ] Ollama Model Size vs. Quality Trade-off
- [ ] Tor Network Stability
- [ ] Rate Limiting Thresholds
- [ ] Database Connection Pool Size

## üìö Dokumentation TODO

- [ ] README.md vervollst√§ndigen
- [ ] API Dokumentation (Swagger erweitern)
- [ ] Deployment Guide schreiben
- [ ] Scraper Configuration Guide
- [ ] Troubleshooting Guide
- [ ] FAQ erstellen
- [ ] Video Tutorials (optional)

## üîÑ Refactoring

- [ ] Scraper Base Class erweitern
- [ ] Error Handling vereinheitlichen
- [ ] Logging konsistent machen
- [ ] Config Management verbessern
- [ ] Code Duplikation reduzieren
- [ ] Type Hints vervollst√§ndigen

## üí° Ideen f√ºr Zukunft

- [ ] Machine Learning f√ºr Lead Scoring
- [ ] Automatische Lead Kategorisierung
- [ ] Sentiment Analysis von Reviews
- [ ] Competitive Intelligence Features
- [ ] Market Analysis Dashboard
- [ ] Predictive Lead Scoring
- [ ] A/B Testing f√ºr Scraping Strategies

---

## üìä Progress Tracking

**Gesamt Fortschritt: ~45%**

- ‚úÖ Infrastructure & Setup (100%)
- ‚úÖ Database Models (100%)
- ‚úÖ Database Migration (100%) ‚≠ê NEW
- ‚úÖ API Endpoints (100%)
- ‚úÖ Data Processors (100%)
- ‚úÖ AI Integration (100%)
- ‚úÖ Unit Testing (85%) ‚≠ê NEW - 47 Tests, 100% Pass Rate
- ‚è≥ Integration Testing (0%)
- ‚è≥ Security (30%)
- ‚è≥ Monitoring (20%)
- ‚è≥ Documentation (50%)
- ‚è≥ Production Deployment (0%)

---

## üéâ HEUTE FERTIG (20.10.2025) - Alle High Priority Features!

### ‚úÖ Phase 1: Backend Features (KOMPLETT)

**1. Webhook Delivery System**
- ‚úÖ HMAC-SHA256 Signaturen f√ºr Sicherheit
- ‚úÖ Retry Logic mit Exponential Backoff (1s, 2s, 4s)
- ‚úÖ Async HTTP Delivery (blockiert API nicht)
- ‚úÖ 3 Events: `job.started`, `job.completed`, `job.failed`
- ‚úÖ Detailliertes Logging & Error Handling
- **Dateien:** `app/utils/webhook_delivery.py`, `app/utils/webhook_helpers.py`
- **Integration:** In `app/api/scraping.py` integriert

**2. Company Deduplicator**
- ‚úÖ 3 Detection Strategies:
  - Exact phone match (100% confidence)
  - Exact website match (95% confidence)
  - Fuzzy name + city matching (85%+ confidence)
- ‚úÖ Smart Merging (keep primary, fill missing fields)
- ‚úÖ Auto-deduplication nach Scraping (95% threshold)
- ‚úÖ API Endpoints: find duplicates, merge, scan all
- ‚úÖ Dry-run mode f√ºr sicheres Testen
- **Dateien:** `app/utils/deduplicator.py`, `app/api/deduplication.py`
- **API Endpoints:**
  - `GET /api/v1/deduplication/companies/{id}/duplicates`
  - `POST /api/v1/deduplication/merge`
  - `POST /api/v1/deduplication/scan`

**3. Alle 6 Scraper aktiviert**
- ‚úÖ 11880.com
- ‚úÖ Gelbe Seiten
- ‚úÖ Das √ñrtliche
- ‚úÖ GoYellow
- ‚úÖ Unternehmensverzeichnis
- ‚úÖ Handelsregister
- **Datei:** `app/api/scraping.py` - Alle Scraper sind jetzt √ºber API verf√ºgbar

### ‚úÖ Phase 2: Dashboard Frontend (KOMPLETT)

**4. React Dashboard mit TypeScript + Vite**
- ‚úÖ **Login Page** - JWT Authentication
- ‚úÖ **Dashboard** - √úbersicht mit Statistiken & Quick Actions
- ‚úÖ **Scraping Page** - Stadt/PLZ eingeben & Scraper starten! üéØ
- ‚úÖ **Companies Page** - Alle Firmen mit Suche, Filter & Pagination

**Features:**
- ‚úÖ Live Job Monitoring (auto-refresh alle 5 Sekunden)
- ‚úÖ 6 Scraper-Quellen Auswahl
- ‚úÖ Responsive Design mit TailwindCSS
- ‚úÖ Lead Quality Badges (A, B, C, D)
- ‚úÖ Pagination f√ºr Company List
- ‚úÖ Real-time Status Updates

**Tech Stack:**
- React 18 + TypeScript
- Vite (schneller Build)
- TailwindCSS (Styling)
- React Query (TanStack Query) - API State Management
- React Router - Navigation
- Axios - HTTP Client
- Lucide Icons

**Dateien:**
- `frontend/src/pages/Login.tsx`
- `frontend/src/pages/Dashboard.tsx`
- `frontend/src/pages/Scraping.tsx` - **HAUPTFEATURE: Stadt/PLZ Input!**
- `frontend/src/pages/Companies.tsx`
- `frontend/src/lib/api.ts` - API Client
- `frontend/src/App.tsx` - Routing

---

## üöÄ MORGEN STARTEN (21.10.2025)

### Schritt 1: Repository pullen
```bash
cd /Users/tobiashaas/Desktop/Lead-Scraper
git pull origin fix/config-extra-fields
```

### Schritt 2: Backend starten
```bash
# Docker Services starten (PostgreSQL, Redis, Ollama)
docker-compose up -d

# Warten bis Services ready sind (ca. 30 Sekunden)
sleep 30

# API starten
make run
# ODER
uvicorn app.main:app --reload
```

**Backend l√§uft auf:** `http://localhost:8000`
**API Docs:** `http://localhost:8000/docs`

### Schritt 3: Frontend starten
```bash
cd frontend

# Dependencies installieren (nur beim ersten Mal)
npm install

# Dev Server starten
npm run dev
```

**Frontend l√§uft auf:** `http://localhost:5173`

### Schritt 4: Testen!

**1. Login:**
- √ñffne `http://localhost:5173`
- Login mit deinem User (oder erstelle einen via API)

**2. Dashboard:**
- Siehst du die Statistiken?
- Funktionieren die Quick Action Buttons?

**3. Scraping Page (HAUPTFEATURE!):**
- Klicke auf "Neuen Scraping-Job starten"
- **Stadt/PLZ eingeben:** z.B. "Stuttgart" oder "70173"
- **Branche eingeben:** z.B. "IT", "Handwerk", "Gastronomie"
- **Quelle w√§hlen:** z.B. "11880.com"
- **Max. Seiten:** z.B. 5
- **Klick auf "Scraping starten"**
- Beobachte die Job-Tabelle unten - sie aktualisiert sich automatisch alle 5 Sekunden!

**4. Companies Page:**
- Siehst du die gescrapten Firmen?
- Funktioniert die Suche?
- Funktioniert die Pagination?

### Schritt 5: Features testen

**Webhook Delivery:**
```bash
# Webhook erstellen (via API Docs oder curl)
curl -X POST "http://localhost:8000/api/v1/webhooks" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://webhook.site/YOUR-UNIQUE-URL",
    "events": ["job.completed", "job.failed"],
    "secret": "my-secret-key",
    "active": true
  }'

# Dann Scraping Job starten und Webhook wird getriggert!
```

**Deduplication:**
```bash
# Duplikate scannen (dry-run)
curl -X POST "http://localhost:8000/api/v1/deduplication/scan?dry_run=true" \
  -H "Authorization: Bearer YOUR_TOKEN"

# Duplikate f√ºr eine Firma finden
curl "http://localhost:8000/api/v1/deduplication/companies/123/duplicates" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

---

## üìù Was zu testen ist:

### ‚úÖ Checklist f√ºr morgen:

**Backend:**
- [ ] API startet ohne Fehler
- [ ] Alle 6 Scraper funktionieren
- [ ] Webhook Delivery funktioniert
- [ ] Deduplication funktioniert
- [ ] Auto-Dedup nach Scraping l√§uft

**Frontend:**
- [ ] Login funktioniert
- [ ] Dashboard zeigt Statistiken
- [ ] Scraping Page: Stadt/PLZ Input funktioniert
- [ ] Scraping Job startet erfolgreich
- [ ] Live Job Monitoring funktioniert (5s refresh)
- [ ] Companies Page zeigt Firmen
- [ ] Suche funktioniert
- [ ] Pagination funktioniert

**Integration:**
- [ ] Frontend ‚Üí Backend Kommunikation
- [ ] Authentication Flow
- [ ] Real-time Updates
- [ ] Error Handling

---

## üêõ Falls Probleme auftreten:

### Backend startet nicht:
```bash
# Logs checken
docker-compose logs

# Services neu starten
docker-compose down
docker-compose up -d

# Database neu initialisieren
make db-init
```

### Frontend startet nicht:
```bash
cd frontend

# Node modules neu installieren
rm -rf node_modules package-lock.json
npm install

# Dev Server starten
npm run dev
```

### API Connection Error:
- Pr√ºfe `.env` Datei im Frontend: `VITE_API_URL=http://localhost:8000/api/v1`
- Pr√ºfe ob Backend l√§uft: `curl http://localhost:8000/health`

### CORS Errors:
- Backend sollte CORS bereits konfiguriert haben
- Falls nicht, in `app/main.py` CORS Middleware pr√ºfen

---

## üéØ N√§chste Schritte (nach Testing):

1. **Feinschliff UI** - Basierend auf deinem Feedback
2. **Mehr Features** - z.B. Webhook Management UI, Deduplication UI
3. **Production Deployment** - Optional
4. **Documentation** - User Guide

---

**Letzte Aktualisierung:** 2025-10-20 21:57 Uhr
**Status:** ‚úÖ Alle High Priority Features fertig!
**N√§chster Schritt:** Testing morgen am Arbeits-PC
