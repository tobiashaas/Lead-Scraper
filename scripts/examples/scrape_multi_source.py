"""
Multi-Source Scraper
Kombiniert Daten aus mehreren BranchenbÃ¼chern
"""

import asyncio
import json
from collections import defaultdict
from pathlib import Path

from app.scrapers.das_oertliche import scrape_das_oertliche
from app.scrapers.eleven_eighty import scrape_11880
from app.scrapers.gelbe_seiten import scrape_gelbe_seiten
from app.scrapers.goyellow import scrape_goyellow
from app.utils.logger import setup_logging


async def main():
    """
    Multi-Source Scraping
    Scraped von mehreren Quellen und kombiniert die Daten
    """

    setup_logging()

    print("=" * 70)
    print("ğŸŒ Multi-Source Lead Scraper")
    print("=" * 70)
    print()

    # Parameter
    city = "Stuttgart"
    industry = "IT-Service"
    max_pages = 1  # Nur 1 Seite pro Quelle fÃ¼r Test
    use_tor = False

    print(f"ğŸ“ Stadt: {city}")
    print(f"ğŸ¢ Branche: {industry}")
    print(f"ğŸ“„ Max Seiten pro Quelle: {max_pages}")
    print()

    # Scrape von allen Quellen
    all_results = []

    sources = [
        ("11880", scrape_11880),
        ("Gelbe Seiten", scrape_gelbe_seiten),
        ("Das Ã–rtliche", scrape_das_oertliche),
        ("GoYellow", scrape_goyellow),
    ]

    for source_name, scrape_func in sources:
        print(f"ğŸ” Scraping {source_name}...")
        try:
            results = await scrape_func(
                city=city, industry=industry, max_pages=max_pages, use_tor=use_tor
            )
            all_results.extend(results)
            print(f"   âœ… {len(results)} Ergebnisse von {source_name}")
        except Exception as e:
            print(f"   âŒ Fehler bei {source_name}: {e}")
        print()

    print("=" * 70)
    print(f"ğŸ“Š Gesamt: {len(all_results)} Ergebnisse von {len(sources)} Quellen")
    print()

    # Deduplizierung nach Firmenname
    print("ğŸ”„ Deduplizierung...")
    companies = defaultdict(list)

    for result in all_results:
        # Normalisiere Firmenname fÃ¼r Matching
        normalized_name = result.company_name.lower().strip()
        companies[normalized_name].append(result)

    print(f"   âœ… {len(companies)} eindeutige Unternehmen gefunden")
    print()

    # Merge Daten von gleichen Unternehmen
    merged_results = []

    for _company_name, results_list in companies.items():
        if len(results_list) == 1:
            # Nur eine Quelle
            merged_results.append(results_list[0])
        else:
            # Mehrere Quellen - merge
            print(f"   ğŸ”— Merge: {results_list[0].company_name} ({len(results_list)} Quellen)")

            # Nehme ersten als Basis
            merged = results_list[0]

            # FÃ¼ge Daten von anderen Quellen hinzu
            for other in results_list[1:]:
                # FÃ¼ge fehlende Daten hinzu
                if not merged.phone and other.phone:
                    merged.phone = other.phone
                if not merged.email and other.email:
                    merged.email = other.email
                if not merged.website and other.website:
                    merged.website = other.website

                # Merge sources
                for source in other.extra_data.get("sources", []):
                    merged.add_source(source["name"], source["url"], source.get("fields", []))

            merged_results.append(merged)

    print()
    print("=" * 70)
    print(f"âœ¨ Finale Ergebnisse: {len(merged_results)} Unternehmen")
    print()

    # Zeige Beispiele
    print("ğŸ“‹ Erste 3 Ergebnisse:")
    print()

    for i, result in enumerate(merged_results[:3], 1):
        print(f"{i}. ğŸ¢ {result.company_name}")
        if result.phone:
            print(f"   ğŸ“ {result.phone}")
        if result.email:
            print(f"   âœ‰ï¸  {result.email}")
        if result.website:
            print(f"   ğŸŒ {result.website}")

        sources = result.extra_data.get("sources", [])
        print(f"   ğŸ“š Quellen: {', '.join([s['name'] for s in sources])}")
        print()

    # Speichern
    output_dir = Path("data/exports")
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / f"multi_source_{city}_{industry}.json"

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump([r.to_dict() for r in merged_results], f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ Gespeichert: {output_file}")
    print()
    print("=" * 70)
    print("âœ… Multi-Source Scraping abgeschlossen!")
    print()

    # Statistik
    source_stats = defaultdict(int)
    for result in merged_results:
        for source in result.extra_data.get("sources", []):
            source_stats[source["name"]] += 1

    print("ğŸ“Š Quellen-Statistik:")
    for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"   - {source}: {count} Unternehmen")


if __name__ == "__main__":
    asyncio.run(main())
