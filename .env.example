# Database Configuration
DATABASE_URL=postgresql://kr_user:kr_password_change_in_production@localhost:5432/kr_leads
DB_ECHO=False  # Set to True for SQL query logging

# Database Connection Pooling
DB_POOL_SIZE=10  # Base pool size (development)
DB_MAX_OVERFLOW=20  # Max overflow connections
DB_POOL_TIMEOUT=30  # Max wait for connection (seconds)
DB_POOL_RECYCLE=3600  # Recycle connections after 1 hour
DB_CONNECT_TIMEOUT=10  # Connection timeout (seconds)
DB_POOL_PRE_PING=True  # Validate connections before use
# Development uses smaller pool (10+20=30 max). Production uses larger pool (20+40=60 max).

# PostgreSQL Credentials (for docker-compose)
POSTGRES_DB=kr_leads
POSTGRES_USER=kr_user
POSTGRES_PASSWORD=your_secure_password_here_change_me

# Redis Configuration
REDIS_URL=redis://localhost:6379
REDIS_DB=0

# RQ Worker Configuration
RQ_WORKER_COUNT=2
RQ_JOB_TIMEOUT=3600
RQ_RESULT_TTL=3600
RQ_FAILURE_TTL=86400

# Tor Configuration
TOR_ENABLED=True
TOR_PROXY=socks5://127.0.0.1:9050
TOR_CONTROL_PORT=9051
TOR_CONTROL_PASSWORD=your_tor_password_here

# Scraping Configuration
SCRAPING_DELAY_MIN=3  # Minimum delay between requests (seconds)
SCRAPING_DELAY_MAX=8  # Maximum delay between requests (seconds)
MAX_RETRIES=3
REQUEST_TIMEOUT=30

# Rate Limiting
RATE_LIMIT_REQUESTS=10  # Max requests per time window
RATE_LIMIT_WINDOW=60    # Time window in seconds

# Playwright Configuration
PLAYWRIGHT_HEADLESS=True
PLAYWRIGHT_BROWSER=firefox  # chromium, firefox, or webkit

# Google Places API
# Get your API key: https://console.cloud.google.com/apis/credentials
# Enable Places API: https://console.cloud.google.com/apis/library/places-backend.googleapis.com
GOOGLE_PLACES_API_KEY=your_google_places_api_key_here

# CAPTCHA Service (optional - nur bei Bedarf)
# TWOCAPTCHA_API_KEY=your_api_key_here
# CAPTCHA_ENABLED=False

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=True  # Set to False in production

# CORS Configuration
# Comma-separated list of allowed origins
# Development: http://localhost:3000,http://localhost:8080
# Production: https://your-domain.com,https://app.your-domain.com
CORS_ORIGINS=http://localhost:3000,http://localhost:8080
CORS_ALLOW_CREDENTIALS=True
CORS_MAX_AGE=600  # Preflight cache duration in seconds

# Security & JWT
SECRET_KEY=your-secret-key-change-in-production-min-32-chars-please-use-strong-random-key
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7

# Logging
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=logs/scraper.log
LOG_MAX_BYTES=10485760  # 10MB
LOG_BACKUP_COUNT=5

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2  # llama3.2, mistral, qwen2.5, etc.
OLLAMA_TIMEOUT=120  # Timeout in seconds

# Ollama Model Selection & Optimization
# Automatically selects the best model based on content complexity and priority
OLLAMA_MODEL_SELECTION_ENABLED=False
OLLAMA_MODEL_PRIORITY=balanced  # speed | accuracy | balanced | resource_efficient
OLLAMA_MODEL_FAST=llama3.2:1b
OLLAMA_MODEL_ACCURATE=llama3.2
OLLAMA_MODEL_BALANCED=mistral
OLLAMA_MODEL_RESOURCE_EFFICIENT=qwen2.5
OLLAMA_BENCHMARK_RESULTS_PATH=data/benchmarks/ollama_results.json
OLLAMA_PROMPT_OPTIMIZATION_ENABLED=False
OLLAMA_PROMPT_LIBRARY_PATH=data/prompts/optimized_prompts.json

# Crawl4AI Configuration
CRAWL4AI_ENABLED=True
CRAWL4AI_WORD_COUNT_THRESHOLD=10
CRAWL4AI_MAX_RETRIES=3

# Smart Scraper Configuration
SMART_SCRAPER_ENABLED=False  # Enable smart scraper globally
SMART_SCRAPER_MODE=enrichment  # Mode: enrichment, fallback, disabled
SMART_SCRAPER_MAX_SITES=10  # Max websites to scrape per job
SMART_SCRAPER_PREFERRED_METHOD=crawl4ai_ollama  # Options: crawl4ai_ollama, trafilatura_ollama, playwright_bs4, httpx_bs4
SMART_SCRAPER_USE_AI=True  # Use AI extraction in smart scraper
SMART_SCRAPER_TIMEOUT=30  # Timeout per website (seconds)

# Deduplicator Configuration
DEDUPLICATOR_ENABLED=True  # Enable automatic duplicate detection
DEDUPLICATOR_REALTIME_ENABLED=True  # Enable real-time duplicate detection during scraping
DEDUPLICATOR_AUTO_MERGE_THRESHOLD=0.95  # Auto-merge duplicates above this similarity (0.0-1.0)
DEDUPLICATOR_CANDIDATE_THRESHOLD=0.80  # Create duplicate candidates above this similarity (0.0-1.0)
DEDUPLICATOR_NAME_THRESHOLD=85  # Minimum name similarity for duplicates (0-100)
DEDUPLICATOR_ADDRESS_THRESHOLD=80  # Minimum address similarity (0-100)
DEDUPLICATOR_PHONE_THRESHOLD=90  # Minimum phone similarity (0-100)
DEDUPLICATOR_WEBSITE_THRESHOLD=95  # Minimum website similarity (0-100)
DEDUPLICATOR_SCAN_SCHEDULE=0 2 * * *  # Cron schedule for duplicate scans (default: 2 AM daily)
DEDUPLICATOR_SCAN_BATCH_SIZE=100  # Batch size for duplicate scans (10-1000)
DEDUPLICATOR_CANDIDATE_RETENTION_DAYS=90  # Days to retain duplicate candidates before cleanup
DEDUPLICATOR_CLEANUP_DELETE_CONFIRMED=False  # Delete confirmed duplicate candidates during cleanup

# Prometheus Metrics
PROMETHEUS_ENABLED=True  # Enable Prometheus metrics collection
PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus_multiproc  # Directory for multiprocess aggregation (required for multiple workers)
METRICS_INCLUDE_LABELS=True  # Include detailed metric labels (set False to reduce cardinality)
METRICS_ENDPOINT_ENABLED=True  # Expose /metrics endpoint for Prometheus scraping

# API Configuration
API_VERSION=1.0.0  # Semantic versioning (MAJOR.MINOR.PATCH - 1.0.0 = stable v1)
API_VERSION_PREFIX=/api/v1  # All endpoints under /api/v1, future /api/v2 for breaking changes
API_BASE_URL=http://localhost:8000  # Base URL (used in docs and webhooks)
API_DEPRECATION_POLICY_URL=https://docs.your-domain.com/api/deprecation

# Alerting & Notifications
ALERTING_ENABLED=False  # Master switch for application alerts

# Email alerts (set ALERT_EMAIL_ENABLED=True to enable)
ALERT_EMAIL_ENABLED=False
ALERT_EMAIL_TO=alerts@example.com  # Comma-separated recipients
ALERT_SMTP_HOST=smtp.sendgrid.net
ALERT_SMTP_PORT=587
ALERT_SMTP_USER=apikey  # Gmail: your@gmail.com | SendGrid: apikey | SES: SMTP username
ALERT_SMTP_PASSWORD=change_me_smtp_password  # Store securely via secrets manager in production
ALERT_SMTP_USE_TLS=True
ALERT_FROM_EMAIL=alerts@lead-scraper.local

# Slack alerts (Incoming Webhook: https://api.slack.com/messaging/webhooks)
ALERT_SLACK_ENABLED=False
ALERT_SLACK_WEBHOOK_URL=https://hooks.slack.com/services/XXX/YYY/ZZZ
ALERT_SLACK_CHANNEL=#kr-alerts  # Optional override. Leave empty to use webhook default
ALERT_SLACK_USERNAME=KR Lead Scraper

# Alertmanager integration (optional)
ALERTMANAGER_URL=http://localhost:9093

# Database Backup Configuration
# Cron format: minute hour day-of-month month day-of-week
BACKUP_ENABLED=True  # Enable automated backups
BACKUP_DAILY_SCHEDULE="0 3 * * *"  # 3 AM daily
BACKUP_WEEKLY_SCHEDULE="0 4 * * 0"  # 4 AM Sunday
BACKUP_MONTHLY_SCHEDULE="0 5 1 * *"  # 5 AM 1st of month
BACKUP_RETENTION_DAILY=7  # Keep last 7 daily backups
BACKUP_RETENTION_WEEKLY=4  # Keep last 4 weekly backups
BACKUP_RETENTION_MONTHLY=12  # Keep last 12 monthly backups (~1 year)
BACKUP_COMPRESSION_ENABLED=True  # Enable gzip compression (5-10x smaller)
BACKUP_ENCRYPTION_ENABLED=False  # Enable GPG encryption (requires key)
BACKUP_ENCRYPTION_KEY=  # GPG key ID (if encryption enabled)
BACKUP_CLOUD_SYNC_ENABLED=False  # Enable cloud sync (S3, GCS, Azure)
BACKUP_CLOUD_PROVIDER=s3  # Cloud provider: s3, gcs, azure
BACKUP_CLOUD_BUCKET=  # Cloud storage bucket name
BACKUP_VERIFICATION_ENABLED=True  # Enable automated verification

# Grafana (local development)
GRAFANA_PASSWORD=admin  # Change in production

# SmartWe Integration (sp√§ter)
# SMARTWE_API_URL=https://api.smartwe.de
# SMARTWE_API_KEY=your_api_key_here

# Development
DEBUG=True
ENVIRONMENT=development  # development, staging, production
