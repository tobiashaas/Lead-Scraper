##############################################
# Production Environment Configuration Template
#
# Copy this file to `.env.production` and replace
# placeholder values before deploying.
##############################################

# --- Environment Basics -----------------------------------------------------
ENVIRONMENT=production
DEBUG=false
APP_NAME="KR Lead Scraper API"
APP_VERSION="1.0.0"

# Set to true to enable verbose SQL logging (recommended: false in prod)
DB_ECHO=false

# --- Secrets Manager --------------------------------------------------------
# Options: none | aws | vault
SECRETS_MANAGER=aws

# AWS Secrets Manager configuration
AWS_REGION=eu-central-1
AWS_PROFILE=
AWS_SECRETS_NAME=kr-lead-scraper/prod

# HashiCorp Vault configuration
VAULT_ADDR=https://vault.example.com
VAULT_TOKEN=
VAULT_PATH=secret/data/kr-lead-scraper/prod

# --- Security & JWT ---------------------------------------------------------
# SECRET_KEY may be loaded from secrets manager when configured.
SECRET_KEY=
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7

# --- Database ---------------------------------------------------------------
# Provide DATABASE_URL or individual POSTGRES_* values.
DATABASE_URL=
POSTGRES_USER=kr_user
POSTGRES_PASSWORD=
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=kr_leads

# Database Connection Pooling (Production)
DB_POOL_SIZE=20  # Larger pool for production
DB_MAX_OVERFLOW=40  # Total: 60 max connections
DB_POOL_TIMEOUT=30  # Max wait for connection
DB_POOL_RECYCLE=3600  # Recycle after 1 hour
DB_CONNECT_TIMEOUT=10  # Connection timeout
DB_POOL_PRE_PING=True  # Always validate connections
# Ensure PostgreSQL max_connections > 60 (default: 100)

# --- Redis ------------------------------------------------------------------
REDIS_URL=redis://:REDIS_PASSWORD_HERE@redis:6379
REDIS_PASSWORD=
REDIS_DB=0

# --- Tor Network ------------------------------------------------------------
TOR_ENABLED=true
TOR_PROXY=socks5://127.0.0.1:9050
TOR_CONTROL_PORT=9051
TOR_CONTROL_PASSWORD=

# --- External APIs ----------------------------------------------------------
GOOGLE_PLACES_API_KEY=
CAPTCHA_ENABLED=false
TWOCAPTCHA_API_KEY=

# --- API Server -------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=false

# --- CORS -------------------------------------------------------------------
CORS_ORIGINS=https://app.example.com
CORS_ALLOW_CREDENTIALS=true
CORS_MAX_AGE=600

# --- Logging ----------------------------------------------------------------
LOG_LEVEL=INFO
LOG_FILE=/var/log/lead-scraper/app.log
LOG_MAX_BYTES=10485760
LOG_BACKUP_COUNT=5

# --- Sentry -----------------------------------------------------------------
SENTRY_ENABLED=true
SENTRY_DSN=
SENTRY_ENVIRONMENT=production
SENTRY_TRACES_SAMPLE_RATE=0.1
SENTRY_PROFILES_SAMPLE_RATE=0.1

# --- Scraping ---------------------------------------------------------------
SCRAPING_DELAY_MIN=3
SCRAPING_DELAY_MAX=8
MAX_RETRIES=3
REQUEST_TIMEOUT=30

# --- Rate Limiting ----------------------------------------------------------
RATE_LIMIT_REQUESTS=10
RATE_LIMIT_WINDOW=60

# --- Playwright -------------------------------------------------------------
PLAYWRIGHT_BROWSER=firefox
PLAYWRIGHT_HEADLESS=true

# --- Ollama -----------------------------------------------------------------
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=llama3.2
OLLAMA_TIMEOUT=120

# --- Crawl4AI ---------------------------------------------------------------
CRAWL4AI_ENABLED=True
CRAWL4AI_WORD_COUNT_THRESHOLD=10
CRAWL4AI_MAX_RETRIES=3

# Smart Scraper Configuration (AI-powered website enrichment)
SMART_SCRAPER_ENABLED=True  # Enable in production for better data quality
SMART_SCRAPER_MODE=enrichment  # enrichment = always enrich, fallback = only if standard fails
SMART_SCRAPER_MAX_SITES=20  # Higher limit for production
SMART_SCRAPER_PREFERRED_METHOD=crawl4ai_ollama
SMART_SCRAPER_USE_AI=True
SMART_SCRAPER_TIMEOUT=45  # Longer timeout for production
# ⚠️ Smart scraper increases job duration but improves data quality significantly
# Recommended: Enable in production, test in staging first

# --- Deduplicator -----------------------------------------------------------
# Automatic duplicate detection and merging
DEDUPLICATOR_ENABLED=True
DEDUPLICATOR_REALTIME_ENABLED=True  # Detect duplicates during scraping
DEDUPLICATOR_AUTO_MERGE_THRESHOLD=0.98  # Stricter threshold for production (98%)
DEDUPLICATOR_CANDIDATE_THRESHOLD=0.85  # Higher threshold for production (85%)
DEDUPLICATOR_NAME_THRESHOLD=90  # Stricter name matching
DEDUPLICATOR_ADDRESS_THRESHOLD=85
DEDUPLICATOR_PHONE_THRESHOLD=95
DEDUPLICATOR_WEBSITE_THRESHOLD=98
DEDUPLICATOR_SCAN_SCHEDULE=0 3 * * *  # 3 AM daily scan
DEDUPLICATOR_SCAN_BATCH_SIZE=200  # Larger batches for production
DEDUPLICATOR_CANDIDATE_RETENTION_DAYS=180  # Keep candidates longer in production
DEDUPLICATOR_CLEANUP_DELETE_CONFIRMED=False  # Preserve confirmed merges

# --- Prometheus Metrics ------------------------------------------------------
PROMETHEUS_ENABLED=True  # Enable Prometheus metrics in production
PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus_multiproc  # Required for multi-worker aggregation
METRICS_INCLUDE_LABELS=True  # Detailed labels for observability (monitor cardinality)
METRICS_ENDPOINT_ENABLED=True

# --- Alerting & Notifications ------------------------------------------------
ALERTING_ENABLED=True

# Email alerts (recommend loading via secrets manager)
ALERT_EMAIL_ENABLED=True
ALERT_EMAIL_TO=alerts@example.com
ALERT_SMTP_HOST=smtp.sendgrid.net  # Gmail: smtp.gmail.com | SES: email-smtp.<region>.amazonaws.com
ALERT_SMTP_PORT=587
ALERT_SMTP_USER=apikey  # Gmail: your@gmail.com | SendGrid: apikey | SES: SMTP username
ALERT_SMTP_PASSWORD=  # Store in secrets manager (e.g., alert_smtp_password)
ALERT_SMTP_USE_TLS=True
ALERT_FROM_EMAIL=alerts@lead-scraper.example

# Slack alerts (Incoming Webhook: https://api.slack.com/messaging/webhooks)
ALERT_SLACK_ENABLED=True
ALERT_SLACK_WEBHOOK_URL=
ALERT_SLACK_CHANNEL=#kr-alerts
ALERT_SLACK_USERNAME=KR Lead Scraper

# Alertmanager integration
ALERTMANAGER_URL=https://alertmanager.example.com

# Database Backup Configuration (Production)
# Cron format: minute hour day-of-month month day-of-week
BACKUP_ENABLED=True  # Enable in production
BACKUP_DAILY_SCHEDULE="0 3 * * *"  # 3 AM daily
BACKUP_WEEKLY_SCHEDULE="0 4 * * 0"  # 4 AM Sunday
BACKUP_MONTHLY_SCHEDULE="0 5 1 * *"  # 5 AM 1st of month
BACKUP_RETENTION_DAILY=7  # 7 days
BACKUP_RETENTION_WEEKLY=4  # 4 weeks
BACKUP_RETENTION_MONTHLY=12  # 12 months
BACKUP_COMPRESSION_ENABLED=True  # Always compress in production
BACKUP_ENCRYPTION_ENABLED=True  # Enable encryption for production
BACKUP_ENCRYPTION_KEY=CHANGE_ME_GPG_KEY_ID  # GPG key ID from secrets
BACKUP_CLOUD_SYNC_ENABLED=True  # Enable cloud sync for disaster recovery
BACKUP_CLOUD_PROVIDER=s3  # AWS S3 for production
BACKUP_CLOUD_BUCKET=kr-scraper-backups-prod  # S3 bucket name
BACKUP_VERIFICATION_ENABLED=True  # Verify backups weekly

# --- Grafana -----------------------------------------------------------------
GRAFANA_PASSWORD=CHANGE_ME_STRONG_PASSWORD  # Store securely (e.g., secrets manager)

# --- Secrets Rotation -------------------------------------------------------
# These values are consumed by scripts in scripts/secrets
BACKUP_ENCRYPTION_KEY=
ROTATION_NOTIFICATION_WEBHOOK=

# --- Checklist --------------------------------------------------------------
# [ ] Set SECRETS_MANAGER to match chosen provider.
# [ ] For AWS: configure AWS_REGION, AWS_SECRETS_NAME, and credentials/role.
# [ ] For Vault: configure VAULT_ADDR, VAULT_TOKEN, VAULT_PATH.
# [ ] Provide SECRET_KEY via secrets manager or directly here.
# [ ] Configure database credentials via secrets manager or POSTGRES_* values.
# [ ] Set third-party API keys (Google Places, 2Captcha, etc.).
# [ ] Configure Sentry DSN and disable API reload for production.
# [ ] Update logging paths to match deployment environment.
# [ ] Remove unused placeholders before committing.
