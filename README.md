# ğŸš€ Lead-Scraper

[![Tests](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/tests.yml/badge.svg)](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/tests.yml)
[![Code Quality](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/code-quality.yml/badge.svg)](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/code-quality.yml)
[![Security](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/security.yml/badge.svg)](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/security.yml)
[![CI/CD](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/ci.yml/badge.svg)](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/ci.yml)
[![Python 3.13](https://img.shields.io/badge/python-3.13-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.118.0-009688.svg)](https://fastapi.tiangolo.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Tests: 99 Passing](https://img.shields.io/badge/tests-99%20passing-brightgreen.svg)](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/tests.yml)
[![Code Coverage](https://img.shields.io/badge/coverage-80%25+-brightgreen.svg)](https://github.com/tobiashaas/Lead-Scraper/actions/workflows/tests.yml)

Automatisiertes Lead-Scraping-System zur Extraktion von Unternehmensdaten aus Baden-WÃ¼rttemberg fÃ¼r IT/BÃ¼rotechnik/Dokumentenmanagement-Leads.

> ğŸ‰ **Production Ready!** Alle Tests passing, Code Quality checks bestanden, Security Scan clean, Docker Build erfolgreich!

## ğŸ“‹ Ãœbersicht

Lead-Scraper ist ein Python-basiertes Web-Scraping-Tool, das Ã¶ffentliche BranchenbÃ¼cher (11880, Gelbe Seiten, etc.) durchsucht und qualitativ hochwertige B2B-Leads fÃ¼r die Region Baden-WÃ¼rttemberg sammelt.

### âœ¨ Features

- ğŸ” **Multi-Source Scraping**: UnterstÃ¼tzung fÃ¼r mehrere BranchenbÃ¼cher
- ğŸ›¡ï¸ **Anti-Detection**: Tor Network Integration + Playwright Stealth Mode
- ğŸ¤– **AI-Powered**: Trafilatura + Ollama fÃ¼r intelligente Datenextraktion (lokal!)
- ğŸ¤– **Smart Scraper**: AI-powered fallback with automatic website enrichment (Crawl4AI + Ollama)
- ğŸ‘¥ **Employee Extraction**: Automatische Extraktion von Mitarbeiterdaten
- ğŸ“Š **Data Quality**: Automatische Validierung, Deduplizierung & Normalisierung
- ğŸ“ˆ **Monitoring & Observability**: Prometheus + Grafana Dashboards inklusive Alerts-Vorbereitung
- ğŸ”„ **Duplicate Detection**: Fuzzy matching with auto-merge and manual review workflows
- ğŸ—„ï¸ **Database**: PostgreSQL mit SQLAlchemy Models
- ğŸ’¾ **Automated Backups**: Geplante PostgreSQL-Backups mit Verifikation & Cloud-Sync
- ğŸ”„ **Rate Limiting**: Intelligentes Request-Management mit Redis
- ğŸ” **Authentication**: JWT-basierte API-Authentifizierung
- ğŸ“ **Structured Logging**: JSON-basiertes Logging mit Correlation IDs
- ğŸ› **Error Tracking**: Sentry Integration fÃ¼r Monitoring
- ğŸ”„ **CI/CD Pipeline**: GitHub Actions fÃ¼r Tests, Code Quality & Security
- ğŸ³ **Docker Ready**: Einfaches Setup mit Docker Compose
- ğŸ“ˆ **REST API**: FastAPI fÃ¼r einfachen Datenzugriff

## ğŸ—ï¸ Architektur

```text
Lead-Scraper/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ scrapers/          # Scraper-Implementierungen
â”‚   â”œâ”€â”€ database/          # SQLAlchemy Models & Migrations
â”‚   â”œâ”€â”€ api/               # FastAPI Endpoints
â”‚   â”œâ”€â”€ utils/             # Proxy Manager, Rate Limiter, Smart Scraper
â”‚   â”œâ”€â”€ processors/        # Data Cleaning & Validation
â”‚   â”œâ”€â”€ workers/           # RQ Workers (Scraping Jobs)
â”‚   â”œâ”€â”€ ai/                # NER & Scoring (spÃ¤ter)
â”‚   â””â”€â”€ core/              # Configuration Management
â”œâ”€â”€ tests/                 # Unit & Integration Tests
â”œâ”€â”€ docs/                  # Dokumentation
â”œâ”€â”€ data/                  # Exports & Temp Files
â”œâ”€â”€ logs/                  # Log Files
â””â”€â”€ config/                # Config Files
```

## ğŸš€ Quick Start

### Voraussetzungen

- **Python 3.11+**
- **Docker & Docker Compose**
- **Tor** (fÃ¼r AnonymitÃ¤t)
- **Git**

### Installation

1. **Repository klonen**

```bash
git clone https://github.com/tobiashaas/Lead-Scraper.git
cd Lead-Scraper
```

1. **Virtual Environment erstellen**

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate    # Windows
```

1. **Dependencies installieren**

```bash
pip install -r requirements.txt
playwright install chromium
```

1. **Tor installieren & starten**

```bash
# macOS
brew install tor
tor

# Linux
sudo apt install tor
sudo systemctl start tor
```

1. **Environment Variables konfigurieren**

```bash
cp .env.example .env
# .env bearbeiten und PasswÃ¶rter anpassen
```

1. **Docker Services starten**

   ```bash
   docker-compose up -d
   ```

1. **Monitoring-Stack starten (optional, empfohlen)**

   ```bash
   make monitoring-up
   # Grafana: http://localhost:3000 (admin / admin)
   # Prometheus: http://localhost:9090
   ```

1. **Worker starten (neues Terminal)**

   ```bash
   make worker
   ```

1. **Ollama Models installieren**

   ```bash
   chmod +x scripts/setup_ollama.sh
   ./scripts/setup_ollama.sh
   ```

1. **Datenbank initialisieren**

   ```bash
   python scripts/init_db.py
   ```

## ğŸ”§ Konfiguration

### Tor Setup (Optional aber empfohlen)

FÃ¼r IP-Rotation Tor Control Port aktivieren:

```bash
# /etc/tor/torrc oder /usr/local/etc/tor/torrc bearbeiten:
ControlPort 9051
HashedControlPassword <generiertes_passwort>

# Passwort generieren:
tor --hash-password "your_password"
```

### Environment Variables

Wichtigste Einstellungen in `.env`:

```bash
# Tor aktivieren/deaktivieren
TOR_ENABLED=True

# Scraping Delays (in Sekunden)
SCRAPING_DELAY_MIN=3
SCRAPING_DELAY_MAX=8

# Rate Limiting
RATE_LIMIT_REQUESTS=10
RATE_LIMIT_WINDOW=60
```

## ğŸ”„ Job Queue & Workers

### Ãœberblick

Scraping-Jobs werden asynchron Ã¼ber **RQ (Redis Queue)** abgearbeitet:

- Die API legt Jobs in Redis ab
- Worker-Prozesse ziehen Jobs von der Queue und fÃ¼hren sie aus
- Fortschritt und Status werden in der Datenbank aktualisiert
- Fehler fÃ¼hren zu automatischen Retries (konfigurierbar)

### Worker lokal starten

```bash
# Virtuelle Umgebung aktivieren und Worker starten
make worker

# Alternativ direkt
./scripts/start_worker.sh

# Windows PowerShell
./scripts/start_worker.ps1
```

## ğŸ’¾ Backups & Restore

### Automatisierte Backups

- RQ Scheduler registriert tÃ¤gliche, wÃ¶chentliche und monatliche Backups.
- Retention Policy sorgt fÃ¼r automatische AufrÃ¤umlÃ¤ufe (daily Ã—7, weekly Ã—4, monthly Ã—12).
- Optionale Features: Gzip-Kompression, GPG-VerschlÃ¼sselung, Cloud Sync (S3/GCS/Azure), IntegritÃ¤tsprÃ¼fung.
- Konfiguration via `.env` (siehe `.env.example`) oder Secrets Manager.

### Manuelle Befehle

```bash
# Backup anstoÃŸen (komprimiert + Verifikation)
make backup-db

# TÃ¤gliches Backup mit Cleanup
make backup-daily

# WÃ¶chentliches Backup mit VerschlÃ¼sselung
make backup-weekly

# Backups auflisten
make backup-list

# Letztes Backup verifizieren
make backup-verify

# Interaktives Restore
make restore-db

# Test-Restore ohne produktive DB zu Ã¼berschreiben
make restore-test
```

### Weitere Infos

- AusfÃ¼hrliche Anleitung: [`docs/BACKUP.md`](docs/BACKUP.md)
- Produktionsleitfaden inkl. Connection Pooling: [`docs/PRODUCTION.md`](docs/PRODUCTION.md)

### Docker-Worker

```bash
# Entwicklung (docker-compose)
make worker-dev

# Produktion (docker-compose.prod.yml)
make worker-prod

# Logs verfolgen
make worker-logs
```

## ğŸ¤– Smart Scraper & AI Enrichment

### Overview

Smart Scraper provides intelligent fallback and enrichment for scraping jobs using AI-powered website analysis.

**Capabilities:**

- Automatic fallback when standard scrapers fail
- Enriches results with data from company websites
- Extracts directors, services, team size, technologies
- Uses local Ollama (no cloud APIs, GDPR-compliant)

### Quick Start

**Enable for a Job:**

```bash
curl -X POST http://localhost:8000/api/v1/scraping/jobs \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "source_name": "11880",
    "city": "Stuttgart",
    "industry": "IT-Service",
    "enable_smart_scraper": true,
    "smart_scraper_mode": "enrichment",
    "max_pages": 3
  }'
```

**Enable Globally:**

```bash
# In .env
SMART_SCRAPER_ENABLED=True
SMART_SCRAPER_MODE=enrichment
SMART_SCRAPER_MAX_SITES=10
```

### Modes

- **Enrichment**: Always enrich results with website data (best quality)
- **Fallback**: Only use smart scraper if standard scraper fails (best performance)
- **Disabled**: Skip smart scraper entirely (fastest)

### Dokumentation

- [AI Scraping Guide](docs/AI-SCRAPING.md): Comprehensive guide
- [Smart Scraper Integration](docs/AI-SCRAPING.md#smart-scraper-integration): Integration details
- [Configuration](docs/AI-SCRAPING.md#configuration): Configuration options
- [Monitoring Guide](docs/MONITORING.md): Prometheus/Grafana Setup & Dashboards

### Model Selection

WÃ¤hle das passende Ollama-Modell abhÃ¤ngig von Durchsatz und Genauigkeit:

- **llama3.2** â€“ HÃ¶chste Feldgenauigkeit, robust fÃ¼r Produktionsjobs.
- **llama3.2:1b** â€“ Schnell und ressourcensparend, ideal fÃ¼r Bulk-LÃ¤ufe.
- **mistral** â€“ Ausgewogene Performance, gute Allround-Wahl.
- **qwen2.5** â€“ Stark bei Kontaktdaten & mehrsprachigen Seiten.

Aktiviere die automatische Auswahl via `.env`:

```bash
OLLAMA_MODEL_SELECTION_ENABLED=true
OLLAMA_MODEL_PRIORITY=llama3.2,llama3.2:1b,mistral,qwen2.5
```

Benchmark- und Optimierungsbefehle:

- `make benchmark-models` â€“ Modelle gegen Testcases messen
- `make benchmark-report` â€“ Markdown-Bericht ausgeben
- `make benchmark-prompts` â€“ Prompt-Optimierung durchfÃ¼hren

Details & Ergebnisse: [docs/AI-SCRAPING.md#model-benchmarks-performance](docs/AI-SCRAPING.md#model-benchmarks-performance)

### Queue-Statistiken & Monitoring

```bash
# Queue-Statistiken ausgeben
make worker-stats

# API Endpoint
curl http://localhost:8000/api/v1/scraping/jobs/stats
```

### Skalierung

- Mehrere Worker-Instanzen kÃ¶nnen parallel laufen
- Docker Compose: `docker-compose up --scale worker=4`
- Produktions-Setup: zusÃ¤tzliche `worker-N` Services ergÃ¤nzen
- Nur ein Worker sollte `--with-scheduler` nutzen, um doppelte geplante Jobs zu vermeiden

## ğŸ“– Verwendung

### API starten (spÃ¤ter)

```bash
uvicorn app.main:app --reload
```

API verfÃ¼gbar unter: `http://localhost:8000`
Docs verfÃ¼gbar unter: `http://localhost:8000/docs`

### Scraping Job starten

```bash
# 11880 Scraper testen
python scrape_11880_test.py
```

Oder programmatisch:

```python
from app.scrapers.eleven_eighty import scrape_11880

results = await scrape_11880(
    city="Stuttgart",
    industry="IT-Service",
    max_pages=2,
    use_tor=False
)
```

## ğŸ§ª Testing

```bash
# Alle Tests ausfÃ¼hren
pytest

# Mit Coverage Report
pytest --cov=app --cov-report=html

# Nur Unit Tests
pytest tests/unit/

# Nur Integration Tests
pytest tests/integration/
```

## ğŸ—ºï¸ Roadmap

### Phase 1: MVP (4-6 Wochen) âœ… In Progress

- [x] Projektstruktur & Setup
- [x] Docker Compose Environment
- [ ] BaseScraper Framework
- [ ] 11880 Scraper
- [ ] Basis ETL Pipeline

### Phase 2: Erweiterung (4-6 Wochen)

- [ ] Gelbe Seiten Scraper
- [ ] Data Validation & Cleaning
- [ ] Deduplizierung
- [ ] FastAPI Endpoints

### Phase 3: AI & Automation (4-6 Wochen)

- [ ] Named Entity Recognition
- [ ] Lead Scoring
- [ ] Automatische Scraping-Jobs
- [ ] SmartWe Integration

### Phase 4: Production (2-4 Wochen)

- [ ] Testing & Bug Fixes
- [ ] Monitoring & Logging
- [ ] VPS Deployment
- [ ] Dokumentation

## âš–ï¸ Rechtliche Hinweise

### Wichtige Hinweise

Dieses Tool scraped Ã¶ffentlich zugÃ¤ngliche Daten aus BranchenbÃ¼chern.

- âœ… **Erlaubt**: Ã–ffentliche BranchenbÃ¼cher (11880, Gelbe Seiten)
- âš ï¸ **Vorsicht**: robots.txt beachten, Rate Limiting einhalten
- âŒ **Verboten**: LinkedIn/Xing Scraping (ToS-VerstoÃŸ)

**Empfehlung**:

- Langsames Scraping (3-8 Sekunden Delay)
- Tor/VPN fÃ¼r AnonymitÃ¤t
- Bei Unsicherheit rechtliche Beratung einholen

## ğŸ¤ Contributing

Contributions sind willkommen! Bitte:

1. Fork das Repository
2. Feature Branch erstellen (`git checkout -b feature/AmazingFeature`)
3. Ã„nderungen committen (`git commit -m 'Add AmazingFeature'`)
4. Branch pushen (`git push origin feature/AmazingFeature`)
5. Pull Request Ã¶ffnen

## ğŸ“ License

Dieses Projekt ist fÃ¼r interne Nutzung bei Kunze & Ritter vorgesehen.

## ğŸ“§ Kontakt

### Kunze & Ritter GmbH

- Website: [kunze-ritter.de](https://kunze-ritter.de)
- GitHub: [@Kunze-Ritter](https://github.com/Kunze-Ritter)

## ğŸ™ Acknowledgments

- [Playwright](https://playwright.dev/) - Browser Automation
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Web Framework
- [Tor Project](https://www.torproject.org/) - Anonymity Network
- [spaCy](https://spacy.io/) - NLP Library

---

**Status**: ğŸš§ In Development - Phase 1 (MVP)

**Ziel**: 10.000+ qualitativ hochwertige Leads/Monat aus Baden-WÃ¼rttemberg
